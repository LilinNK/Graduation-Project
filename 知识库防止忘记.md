## æ's çŸ¥è¯†åº“

### 0. å¸¸ç”¨é“¾æ¥

  ğŸ¤–[NLPå¤§ä½œä¸šå‚è€ƒ](https://github.com/yangxze/ChatGLM-LangChain)
  ğŸ˜»[å°å¤§æå¼˜æ¯…è€å¸ˆçš„è¯¾ç¨‹ä¸»é¡µ](https://speech.ee.ntu.edu.tw/~hylee/index.php)
  âš›[openAI](https://openai.com/)
  ğŸ¤—[Hugging Face](https://huggingface.co/)


### 1. Hugging Faceï¼ˆtransformersğŸ¤—ï¼‰

#### 1.1 ç›¸å…³å†™çš„ä¸é”™çš„å…¥é—¨ç¬”è®°ï¼š

1.1.1 [æŒ‡å¯¼ç¬”è®°ä¸Šç¯‡](https://zhuanlan.zhihu.com/p/448852278)  
  - ä¸»è¦å†…å®¹
    
    **å®‰è£…**
    ```pyhton
    #ç›´æ¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ä¼šå¥½ä¸€äº›ï¼Œå¤šå®‰è£…ä¾èµ–åº“ï¼šsentencepiece å’Œ protobuf
    !pip install transformers[sentencepiece]
    ```
    **pipelineèƒŒåçš„æµç¨‹(å€¼å¾—å¼„æ‡‚ï¼)**
    <img width="1774" alt="image" src="https://github.com/LilinNK/Graduation-Project/assets/96948927/9906abb2-de06-4533-be96-0937d8459a6c">

    <img width="1767" alt="image" src="https://github.com/LilinNK/Graduation-Project/assets/96948927/48e47e1e-2e54-4a2b-9ec5-31306591ccb8">

    **Tokenizer:** [AutoTokenizer](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/auto#transformers.AutoTokenizer)
    ```
    from transformers import AutoTokenizer

    # Download vocabulary from huggingface.co and cache.
    tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
    
    # Download vocabulary from huggingface.co (user-uploaded) and cache.
    tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")
    
    # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
    # tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")
    
    # Download vocabulary from huggingface.co and define model-specific arguments
    tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-base", add_prefix_space=True) #add_prefix_space æŒ‡åœ¨ç¼–ç æ–‡æœ¬æ—¶æ˜¯å¦åœ¨è¯ä¹‹å‰æ·»åŠ ä¸€ä¸ªç©ºæ ¼
    ```
    **Models**
    ```
    from transformers import AutoConfig, AutoModel
    
    # Download model and configuration from huggingface.co and cache.
    model = AutoModel.from_pretrained("google-bert/bert-base-cased")
    
    # Update configuration during loading
    model = AutoModel.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
    model.config.output_attentions
    
    # Loading from a TF checkpoint file instead of a PyTorch model (slower)
    config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
    model = AutoModel.from_pretrained(
        "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
    )
    ```
    **å¯¹åº”çš„é¢„è®­ç»ƒæ–‡æ¡£ä¸ºï¼š**
    
    config:æ¨¡å‹æ¶æ„æ–‡ä»¶ã€‚

    pytorch_model.bin(tf_model.h5,flax_model.msgpack,model.safetensors):æ¨¡å‹å…·ä½“å‚æ•°æ–‡ä»¶ã€‚

    vocab(merges,special_tokens_map,tokenizer,tokenizer_config):åˆ†è¯é¢„å¤„ç†çš„æ–‡ä»¶ã€‚

    eg:
    Bert: vocab.txt => Roberta: merge.txt+vocab.json åˆ†è¯ç¼–ç ä¸åŒBertï¼šBPEç¼–ç ï¼ŒRobertaï¼šbyte levelçš„BPE(BBPE)[å‚çœ‹](https://blog.csdn.net/ljp1919/article/details/113616226)


1.1.2 å¸¸ç”¨å®‰è£…å‘½ä»¤

    ```
    pip install transformers[sentencepiece]
    
    pip install datasets
    
    pip install evaluate
    
    pip install pandas
    
    pip install torch
    
    pip install peft
    
    pip install scikit-learn
    
    **pip install -help**  #å¯ä»¥æŸ¥çœ‹å„ç§pipå‚æ•°
    
    pip install -r requirement.txt
    
    pip install -u scikit-learn  #å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬
    
    pip install -q transformers  #å®‰é™å®‰è£…ï¼Œä¸è¾“å‡ºå®‰è£…ä¿¡æ¯ï¼Œåªè¾“å‡ºæˆåŠŸ/å¤±è´¥
    ```
1.1.3 å¸¸ç”¨å‡½æ•°å¯¼å…¥

    ```
    from sklearn.metrics import f1_score, accuracy_score, classification_report

    from transformers import RobertaModel, BertModel, RobertaConfig,DebertaModel,DebertaConfig,DebertaV2Model,DebertaV2Config

    
    ```

### 2. Linux

#### 2.1 å¸¸ç”¨å‘½ä»¤è¡Œ



