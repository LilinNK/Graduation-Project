## 李's 知识库

### 0. 常用链接

  🤖[NLP大作业参考](https://github.com/yangxze/ChatGLM-LangChain)
  😻[台大李弘毅老师的课程主页](https://speech.ee.ntu.edu.tw/~hylee/index.php)
  ⚛[openAI](https://openai.com/)
  🤗[Hugging Face](https://huggingface.co/)


### 1. Hugging Face（transformers🤗）

#### 1.1 相关写的不错的入门笔记：

1.1.1 [指导笔记上篇](https://zhuanlan.zhihu.com/p/448852278)  
  - 主要内容
    
    **安装**
    ```pyhton
    #直接使用如下命令安装会好一些，多安装依赖库：sentencepiece 和 protobuf
    !pip install transformers[sentencepiece]
    ```
    **pipeline背后的流程(值得弄懂！)**
    <img width="1774" alt="image" src="https://github.com/LilinNK/Graduation-Project/assets/96948927/9906abb2-de06-4533-be96-0937d8459a6c">

    <img width="1767" alt="image" src="https://github.com/LilinNK/Graduation-Project/assets/96948927/48e47e1e-2e54-4a2b-9ec5-31306591ccb8">

    **Tokenizer:** [AutoTokenizer](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/auto#transformers.AutoTokenizer)
    ```
    from transformers import AutoTokenizer

    # Download vocabulary from huggingface.co and cache.
    tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
    
    # Download vocabulary from huggingface.co (user-uploaded) and cache.
    tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")
    
    # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
    # tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")
    
    # Download vocabulary from huggingface.co and define model-specific arguments
    tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-base", add_prefix_space=True) #add_prefix_space 指在编码文本时是否在词之前添加一个空格
    ```
    **Models**
    ```
    from transformers import AutoConfig, AutoModel
    
    # Download model and configuration from huggingface.co and cache.
    model = AutoModel.from_pretrained("google-bert/bert-base-cased")
    
    # Update configuration during loading
    model = AutoModel.from_pretrained("google-bert/bert-base-cased", output_attentions=True)
    model.config.output_attentions
    
    # Loading from a TF checkpoint file instead of a PyTorch model (slower)
    config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
    model = AutoModel.from_pretrained(
        "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
    )
    ```
    **对应的预训练文档为：**
    
    config:模型架构文件。

    pytorch_model.bin(tf_model.h5,flax_model.msgpack,model.safetensors):模型具体参数文件。

    vocab(merges,special_tokens_map,tokenizer,tokenizer_config):分词预处理的文件。

    eg:
    Bert: vocab.txt => Roberta: merge.txt+vocab.json 分词编码不同Bert：BPE编码，Roberta：byte level的BPE(BBPE)[参看](https://blog.csdn.net/ljp1919/article/details/113616226)


1.1.2 常用安装命令

    ```
    pip install transformers[sentencepiece]
    
    pip install datasets
    
    pip install evaluate
    
    pip install pandas
    
    pip install torch
    
    pip install peft
    
    pip install scikit-learn
    
    **pip install -help**  #可以查看各种pip参数
    
    pip install -r requirement.txt
    
    pip install -u scikit-learn  #升级到最新版本
    
    pip install -q transformers  #安静安装，不输出安装信息，只输出成功/失败
    ```
1.1.3 常用函数导入

    ```
    from sklearn.metrics import f1_score, accuracy_score, classification_report

    from transformers import RobertaModel, BertModel, RobertaConfig,DebertaModel,DebertaConfig,DebertaV2Model,DebertaV2Config

    
    ```

### 2. Linux

#### 2.1 常用命令行



