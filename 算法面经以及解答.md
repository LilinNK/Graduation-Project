### 小红书面经🥺
#### 饿了么LLM算法面经
- lora怎么初始化参数？为什么一半是0？

  LoRA（Low-Rank Adaptation）是针对大型语言模型的一种参数高效微调方法。LoRA通过插入低秩矩阵到预训练模型的特定层中，使得在微调时只需要更新少量的参数，从而减少计算资源和存储需求。
  
  LoRA通过插入两个低秩矩阵A和B实现参数高效微调，其中矩阵B初始化为零矩阵，矩阵A使用正态分布进行初始化。
  
  这种初始化策略主要是为了
  1. 初始权重的稳定性：避免了模型在训练初期由于权重大幅改变而导致的不稳定。
  2. 参数稀疏性：在训练初期，参数更新是稀疏的，有助于降低计算复杂度和内存使用。
  
  保持模型训练初期的稳定性，减少参数更新的幅度，并逐步引导模型找到更好的优化路径。这种方式不仅降低了计算资源的需求，还能提高模型的微调效率。
