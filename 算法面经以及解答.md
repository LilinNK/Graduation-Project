### 小红书面经🥺
#### 饿了么LLM算法面经
- lora怎么初始化参数？为什么一半是0？

  LoRA（Low-Rank Adaptation）是针对大型语言模型的一种参数高效微调方法。LoRA通过插入低秩矩阵到预训练模型的特定层中，使得在微调时只需要更新少量的参数，从而减少计算资源和存储需求。
  
  LoRA通过插入两个低秩矩阵A和B实现参数高效微调，其中矩阵B初始化为零矩阵，矩阵A使用正态分布进行初始化。
  
  这种初始化策略主要是为了
  1. 初始权重的稳定性：避免了模型在训练初期由于权重大幅改变而导致的不稳定。
  2. 参数稀疏性：在训练初期，参数更新是稀疏的，有助于降低计算复杂度和内存使用。
  
  保持模型训练初期的稳定性，减少参数更新的幅度，并逐步引导模型找到更好的优化路径。这种方式不仅降低了计算资源的需求，还能提高模型的微调效率。
- post-norm和pre-norm的区别和各自的优势

  在神经网络，特别是Transformer架构中，`pre-norm` 和 `post-norm` 是两种不同的Layer Normalization（层归一化）应用策略。Layer Normalization 在Transformer中主要用于稳定训练过程并加速收敛。两种策略的区别在于Layer Normalization的位置：是放在残差连接（Residual Connection）之前还是之后。post-norm在残差之后做归一化，对参数正则化的效果更强，进而模型的**鲁棒性**也会更好；pre-norm相对于post-norm，因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以**防止模型的梯度爆炸或者梯度消失**。

  ##### 详细示例
  <img width="1002" alt="image" src="https://github.com/LilinNK/Graduation-Project/assets/96948927/d6ba042b-b0e3-4b98-a463-3e68189f3f38">

  ##### Pre-Norm
  
  在Pre-Norm架构中，每个子层的输入在应用子层之前都进行Layer Normalization。
  
  ```python
  def pre_norm_layer(x, sublayer):
      return x + sublayer(LayerNorm(x))
  # 结构：LayerNorm → Sublayer → Residual Connection
  
  # Transformer块的简化结构
  def transformer_block_pre_norm(x, attention_layer, feed_forward_layer):
      x = pre_norm_layer(x, attention_layer)
      x = pre_norm_layer(x, feed_forward_layer)
      return x
  ```
  
  ##### Post-Norm
  
  在Post-Norm架构中，每个子层的输出在应用子层之后进行Layer Normalization。
  
  ```python
  def post_norm_layer(x, sublayer):
      return LayerNorm(x + sublayer(x))
  # 结构：Sublayer → Residual Connection → LayerNorm
  
  # Transformer块的简化结构
  def transformer_block_post_norm(x, attention_layer, feed_forward_layer):
      x = post_norm_layer(x, attention_layer)
      x = post_norm_layer(x, feed_forward_layer)
      return x
  ```
  
  ##### 实践中的选择
  
  - **Pre-Norm**：通常用于较深的Transformer架构，尤其是在需要确保训练稳定性的情况下，比如在自然语言处理（NLP）任务中的BERT等模型中。
  - **Post-Norm**：常见于较浅的模型，或是在需要更高灵活性的任务中。

  ##### 总结

  Pre-Norm：优势：稳定性高，梯度流动平滑。劣势：可能限制模型的表达能力，训练需要更多调试。

  Post-Norm：优势：表达能力强，实施简单。劣势：训练初期不稳定，收敛难度较大。

  😊[参考](https://zhuanlan.zhihu.com/p/474988236)
