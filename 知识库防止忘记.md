## æ's çŸ¥è¯†åº“

### 0. å¸¸ç”¨é“¾æ¥

  ğŸ¤–[NLPå¤§ä½œä¸šå‚è€ƒ](https://github.com/yangxze/ChatGLM-LangChain)
  ğŸ˜»[å°å¤§æå¼˜æ¯…è€å¸ˆçš„è¯¾ç¨‹ä¸»é¡µ](https://speech.ee.ntu.edu.tw/~hylee/index.php)
  âš›[openAI](https://openai.com/)
  ğŸ¤—[Hugging Face](https://huggingface.co/)
  ğŸ–Œï¸[åšå›¾è½¯ä»¶](https://www.canva.cn/)


### 1. Hugging Faceï¼ˆtransformersğŸ¤—ï¼‰

#### 1.1 ç›¸å…³å†™çš„ä¸é”™çš„å…¥é—¨ç¬”è®°ï¼š

1.1.1 [æŒ‡å¯¼ç¬”è®°ä¸Šç¯‡](https://zhuanlan.zhihu.com/p/448852278)  
  - ä¸»è¦å†…å®¹
    
    **å®‰è£…**
    ```pyhton
    #ç›´æ¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ä¼šå¥½ä¸€äº›ï¼Œå¤šå®‰è£…ä¾èµ–åº“ï¼šsentencepiece å’Œ protobuf
    !pip install transformers[sentencepiece]
    ```
    **pipelineèƒŒåçš„æµç¨‹(å€¼å¾—å¼„æ‡‚ï¼)**
    <img width="1774" alt="image" src="https://github.com/LilinNK/Graduation-Project/assets/96948927/9906abb2-de06-4533-be96-0937d8459a6c">

    <img width="1767" alt="image" src="https://github.com/LilinNK/Graduation-Project/assets/96948927/48e47e1e-2e54-4a2b-9ec5-31306591ccb8">

    **Tokenizer:** [AutoTokenizer](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/auto#transformers.AutoTokenizer)
    ```
    from transformers import AutoTokenizer

    # Download vocabulary from huggingface.co and cache.
    tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
    
    # Download vocabulary from huggingface.co (user-uploaded) and cache.
    tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")
    
    # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
    # tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")
    
    # Download vocabulary from huggingface.co and define model-specific arguments
    tokenizer = AutoTokenizer.from_pretrained("FacebookAI/roberta-base", add_prefix_space=True) #add_prefix_space æŒ‡åœ¨ç¼–ç æ–‡æœ¬æ—¶æ˜¯å¦åœ¨è¯ä¹‹å‰æ·»åŠ ä¸€ä¸ªç©ºæ ¼
    ```
    **Models**
    ```
    from transformers import AutoConfig, AutoModel
    
    # Download model and configuration from huggingface.co and cache.
    model = AutoModel.from_pretrained("google-bert/bert-base-cased")
    
    # Update configuration during loading
    model = AutoModel.from_pretrained("google-bert/bert-base-cased", output_attentions=True)  #ç»™å®šè¾“å…¥ï¼Œä¼šæœ‰æ³¨æ„åŠ›æƒé‡å¯ä»¥è¾“å‡º
    model.config.output_attentions
    
    # Loading from a TF checkpoint file instead of a PyTorch model (slower)
    config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
    model = AutoModel.from_pretrained(
        "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
    )
    ```
    **å¯¹åº”çš„é¢„è®­ç»ƒæ–‡æ¡£ä¸ºï¼š**
    
    config:æ¨¡å‹æ¶æ„æ–‡ä»¶ã€‚

    pytorch_model.bin(tf_model.h5,flax_model.msgpack,model.safetensors):æ¨¡å‹å…·ä½“å‚æ•°æ–‡ä»¶ã€‚

    vocab(merges,special_tokens_map,tokenizer,tokenizer_config):åˆ†è¯é¢„å¤„ç†çš„æ–‡ä»¶ã€‚

    eg:
    Bert: vocab.txt => Roberta: merge.txt+vocab.json åˆ†è¯ç¼–ç ä¸åŒ  Bertï¼šBPEç¼–ç ï¼ŒRobertaï¼šbyte levelçš„BPE(BBPE)[å‚çœ‹](https://blog.csdn.net/ljp1919/article/details/113616226)


1.1.2 å¸¸ç”¨å®‰è£…å‘½ä»¤

    ```
    pip install transformers[sentencepiece]
    
    pip install datasets
    
    pip install evaluate
    
    pip install pandas
    
    pip install torch
    
    pip install peft
    
    pip install scikit-learn
    
    **pip install -help**  #å¯ä»¥æŸ¥çœ‹å„ç§pipå‚æ•°
    
    pip install -r requirement.txt
    
    pip install -u scikit-learn  #å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬
    
    pip install -q transformers  #å®‰é™å®‰è£…ï¼Œä¸è¾“å‡ºå®‰è£…ä¿¡æ¯ï¼Œåªè¾“å‡ºæˆåŠŸ/å¤±è´¥

    pip list   #è¿è¡Œè¿™ä¸ªå‘½ä»¤ï¼Œå®ƒä¼šåˆ—å‡ºå½“å‰Pythonç¯å¢ƒä¸­å®‰è£…çš„æ‰€æœ‰åŒ…åŠå…¶ç‰ˆæœ¬å·
    
    pip show package_name   #æŸ¥çœ‹ç‰¹å®šåŒ…çš„ç‰ˆæœ¬
    
    ```
#### 1.2 å¸¸ç”¨å‡½æ•°å¯¼å…¥

    ```
    from sklearn.metrics import f1_score, accuracy_score, classification_report

    from transformers import RobertaModel, BertModel, RobertaConfig,DebertaModel,DebertaConfig,DebertaV2Model,DebertaV2Config

    
    ```

### 2. Linux

#### 2.1 å¸¸ç”¨å‘½ä»¤è¡Œï¼ˆ[å‚è€ƒ](https://www.autodl.com/docs/linux/)ï¼‰

  ```
  #æŸ¥çœ‹ç›®å½•
  ls  #åˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
  ls -l  #åˆ—å‡ºæ–‡ä»¶å’Œæ–‡ä»¶å¤¹çš„è¯¦ç»†ä¿¡æ¯ï¼šæƒé™ï¼ŒOwnerï¼ŒGroupå’Œåˆ›å»º/æ›´æ–°æ—¶é—´
  ls -a  #åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ï¼ŒåŒ…æ‹¬.å¼€å¤´çš„éšè—æ–‡ä»¶

  #é‡å‘½åã€åˆ›å»ºã€ç§»åŠ¨æ–‡ä»¶/æ–‡ä»¶å¤¹
  touch newfile.txt    #åˆ›å»ºæ–‡ä»¶
  mkdir test_dir #åˆ›å»ºæ–°è·¯å¾„
  mv test_dir/ test_directory #é‡å‘½å
  mv a b/  #æŠŠaç§»åŠ¨åˆ°bç›®å½•ä¸‹ï¼Œå¦‚æœbç›®å½•ä¸å­˜åœ¨åˆ™å˜ä¸ºé‡å‘½å
  cp -r a b   #å°†aæ–‡ä»¶å¤¹æ‹·è´åˆ°bæ–‡ä»¶å¤¹ä¸‹ï¼Œ-rä»£è¡¨é€’å½’æ‹·è´
  rm -rf folder  #åˆ é™¤æ–‡ä»¶/æ–‡ä»¶å¤¹
  rm -rf folder/*   # *æ˜¯é€šé…ç¬¦å·ï¼Œè¿™æ ·ä»£è¡¨folderæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶/æ–‡ä»¶å¤¹

  #æŸ¥çœ‹è·¯å¾„
  cd   #åˆ‡æ¢è·¯å¾„
  pwd  #æŸ¥çœ‹å·¥ä½œè·¯å¾„
  .. or ../  #ä¸Šä¸€çº§ç›®å½•
  . or ./  #åŒçº§ç›®å½•

  #æŸ¥çœ‹gpu
  nvidia-smi #æŸ¥çœ‹æ­¤æ—¶æƒ…å†µ
  nvidia-smi -l 1 / watch -n 1 nvidia-smi #æ¯éš”1ç§’è¾“å‡ºä¸€æ¬¡

  #æŸ¥çœ‹è¿›ç¨‹
  ps -ef
  #-e é€‰é¡¹è¡¨ç¤ºæ˜¾ç¤ºæ‰€æœ‰è¿›ç¨‹ï¼Œè€Œä¸ä»…ä»…æ˜¯ç”±å½“å‰ç”¨æˆ·æ‹¥æœ‰çš„è¿›ç¨‹ã€‚
  #-f é€‰é¡¹è¡¨ç¤ºæ˜¾ç¤ºå®Œæ•´æ ¼å¼ï¼Œè¿™é€šå¸¸åŒ…æ‹¬è¿›ç¨‹ ID (PID)ã€çˆ¶è¿›ç¨‹ ID (PPID)ã€ç”¨æˆ· ID (UID)ã€å¯åŠ¨æ—¶é—´ã€æ§åˆ¶ç»ˆç«¯ã€CPU ä½¿ç”¨æ—¶é—´ä»¥åŠå‘½ä»¤åç§°ç­‰ä¿¡æ¯ã€‚

  #æ€æ­»è¿›ç¨‹
  kill -9 402  #402æ˜¯è¿›ç¨‹ID PID
  ```

